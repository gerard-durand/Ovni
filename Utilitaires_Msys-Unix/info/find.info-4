This is /devshop/msys/rt-HEAD/packages/findutils/4.3/doc/find.info,
produced by makeinfo version 4.3 from
/devshop/msys/rt-HEAD/packages/findutils/4.3/doc/find.texi.

INFO-DIR-SECTION Basics
START-INFO-DIR-ENTRY
* Finding files: (find).        Operating on files matching certain criteria.
END-INFO-DIR-ENTRY

INFO-DIR-SECTION Individual utilities
START-INFO-DIR-ENTRY
* find: (find)Invoking find.                    Finding and acting on files.
* locate: (find)Invoking locate.                Finding files in a database.
* updatedb: (find)Invoking updatedb.            Building the locate database.
* xargs: (find)Invoking xargs.                  Operating on many files.
END-INFO-DIR-ENTRY


File: find.info,  Node: posix-extended regular expression syntax,  Prev: posix-egrep regular expression syntax,  Up: Regular Expressions

`posix-extended' regular expression syntax
------------------------------------------

   The character `.' matches any single character except the null
character.

`+'
     indicates that the regular expression should match one or more
     occurrences of the previous atom or regexp.

`?'
     indicates that the regular expression should match zero or one
     occurrence of the previous atom or regexp.

`\+'
     matches a `+'

`\?'
     matches a `?'.

   Bracket expressions are used to match ranges of characters.  Bracket
expressions where the range is backward, for example `[z-a]', are
invalid.  Within square brackets, `\' is taken literally.  Character
classes are supported; for example `[[:digit:]]' will match a single
decimal digit.

   GNU extensions are supported:
  1. `\w' matches a character within a word

  2. `\W' matches a character which is not within a word

  3. `\<' matches the beginning of a word

  4. `\>' matches the end of a word

  5. `\b' matches a word boundary

  6. `\B' matches characters which are not a word boundary

  7. `\`' matches the beginning of the whole input

  8. `\'' matches the end of the whole input


   Grouping is performed with parentheses `()'.  An unmatched `)'
matches just itself.  A backslash followed by a digit acts as a
back-reference and matches the same thing as the previous grouped
expression indicated by that number.  For example `\2' matches the
second group expression.  The order of group expressions is determined
by the position of their opening parenthesis `('.

   The alternation operator is `|'.

   The characters `^' and `$' always represent the beginning and end of
a string respectively, except within square brackets.  Within brackets,
`^' can be used to invert the membership of the character class being
specified.

   `*', `+' and `?' are special at any point in a regular expression
except the following places, where they are not allowed:
  1. At the beginning of a regular expression

  2. After an open-group, signified by `('

  3. After the alternation operator `|'


   Intervals are specified by `{' and `}'.  Invalid intervals such as
`a{1z' are not accepted.

   The longest possible match is returned; this applies to the regular
expression as a whole and (subject to this constraint) to
subexpressions within groups.


File: find.info,  Node: Environment Variables,  Prev: Regular Expressions,  Up: Reference

Environment Variables
=====================

LANG
     Provides a default value for the internationalisation variables
     that are unset or null.

LC_ALL
     If set to a non-empty string value, override the values of all the
     other internationalisation variables.

LC_COLLATE
     The POSIX standard specifies that this variable affects the pattern
     matching to be used for the `\-name' option.  GNU find uses the
     GNU version of the `fnmatch' library function.

     POSIX also specifies that the `LC_COLLATE' environment variable
     affects the interpretation of the user's response to the query
     issued by `\-ok', but this is not the case for GNU find.

LC_CTYPE
     This variable affects the treatment of character classes used with
     the `-name' test, if the system's `fnmatch' library function
     supports this.   It has no effect on the behaviour of the `-ok'
     expression.

LC_MESSAGES
     Determines the locale to be used for internationalised messages.

NLSPATH
     Determines the location of the internationalisation message
     catalogues.

PATH
     Affects the directories which are searched to find the executables
     invoked by `-exec', `-execdir' `-ok' and `-okdir'.  If the PATH
     environment variable includes the current directory (by explicitly
     including `.' or by having an empty element), and the find command
     line includes `-execdir' or `-okdir', `find' will refuse to run.
     *Note Security Considerations::, for a more detailed discussion of
     security matters.

POSIXLY_CORRECT
     Determines the block size used by `-ls' and `-fls'.  If
     POSIXLY_CORRECT is set, blocks are units of 512 bytes.  Otherwise
     they are units of 1024 bytes.

TZ
     Affects the time zone used for some of the time-related format
     directives of `-printf' and `-fprintf'.


File: find.info,  Node: Common Tasks,  Next: Worked Examples,  Prev: Reference,  Up: Top

Common Tasks
************

   The sections that follow contain some extended examples that both
give a good idea of the power of these programs, and show you how to
solve common real-world problems.

* Menu:

* Viewing And Editing::
* Archiving::
* Cleaning Up::
* Strange File Names::
* Fixing Permissions::
* Classifying Files::


File: find.info,  Node: Viewing And Editing,  Next: Archiving,  Up: Common Tasks

Viewing And Editing
===================

   To view a list of files that meet certain criteria, simply run your
file viewing program with the file names as arguments.  Shells
substitute a command enclosed in backquotes with its output, so the
whole command looks like this:

     less `find /usr/include -name '*.h' | xargs grep -l mode_t`

You can edit those files by giving an editor name instead of a file
viewing program:

     emacs `find /usr/include -name '*.h' | xargs grep -l mode_t`

   Because there is a limit to the length of any individual command
line, there is a limit to the number of files that can be handled in
this way.  We can get around this difficulty by using xargs like this:

     find /usr/include -name '*.h' | xargs grep -l mode_t > todo
     xargs --arg-file=todo emacs

   Here, `xargs' will run `emacs' as many times as necessary to visit
all of the files listed in the file `todo'.


File: find.info,  Node: Archiving,  Next: Cleaning Up,  Prev: Viewing And Editing,  Up: Common Tasks

Archiving
=========

   You can pass a list of files produced by `find' to a file archiving
program.  GNU `tar' and `cpio' can both read lists of file names from
the standard input--either delimited by nulls (the safe way) or by
blanks (the lazy, risky default way).  To use null-delimited names,
give them the `--null' option.  You can store a file archive in a file,
write it on a tape, or send it over a network to extract on another
machine.

   One common use of `find' to archive files is to send a list of the
files in a directory tree to `cpio'.  Use `-depth' so if a directory
does not have write permission for its owner, its contents can still be
restored from the archive since the directory's permissions are
restored after its contents.  Here is an example of doing this using
`cpio'; you could use a more complex `find' expression to archive only
certain files.

     find . -depth -print0 |
       cpio --create --null --format=crc --file=/dev/nrst0

   You could restore that archive using this command:

     cpio --extract --null --make-dir --unconditional \
       --preserve --file=/dev/nrst0

   Here are the commands to do the same things using `tar':

     find . -depth -print0 |
       tar --create --null --files-from=- --file=/dev/nrst0
     
     tar --extract --null --preserve-perm --same-owner \
       --file=/dev/nrst0

   Here is an example of copying a directory from one machine to
another:

     find . -depth -print0 | cpio -0o -Hnewc |
       rsh OTHER-MACHINE "cd `pwd` && cpio -i0dum"


File: find.info,  Node: Cleaning Up,  Next: Strange File Names,  Prev: Archiving,  Up: Common Tasks

Cleaning Up
===========

   This section gives examples of removing unwanted files in various
situations.  Here is a command to remove the CVS backup files created
when an update requires a merge:

     find . -name '.#*' -print0 | xargs -0r rm -f

   The command above works, but the following is safer:

     find . -name '.#*' -depth -delete

   You can run this command to clean out your clutter in `/tmp'.  You
might place it in the file your shell runs when you log out
(`.bash_logout', `.logout', or `.zlogout', depending on which shell you
use).

     find /tmp -depth -user "$LOGNAME" -type f -delete

   If your `find' command removes directories, you may find that you
get a spurious error message when `find' tries to recurse into a
directory that has now been removed.  Using the `-depth' option will
normally resolve this problem.

   To remove old Emacs backup and auto-save files, you can use a command
like the following.  It is especially important in this case to use
null-terminated file names because Emacs packages like the VM mailer
often create temporary file names with spaces in them, like `#reply to
David J. MacKenzie<1>#'.

     find ~ \( -name '*~' -o -name '#*#' \) -print0 |
       xargs --no-run-if-empty --null rm -vf

   Removing old files from `/tmp' is commonly done from `cron':

     find /tmp /var/tmp -not -type d -mtime +3 -delete
     find /tmp /var/tmp -depth -mindepth 1 -type d -empty -delete

   The second `find' command above uses `-depth' so it cleans out empty
directories depth-first, hoping that the parents become empty and can
be removed too.  It uses `-mindepth' to avoid removing `/tmp' itself if
it becomes totally empty.


File: find.info,  Node: Strange File Names,  Next: Fixing Permissions,  Prev: Cleaning Up,  Up: Common Tasks

Strange File Names
==================

   `find' can help you remove or rename a file with strange characters
in its name.  People are sometimes stymied by files whose names contain
characters such as spaces, tabs, control characters, or characters with
the high bit set.  The simplest way to remove such files is:

     rm -i SOME*PATTERN*THAT*MATCHES*THE*PROBLEM*FILE

   `rm' asks you whether to remove each file matching the given
pattern.  If you are using an old shell, this approach might not work
if the file name contains a character with the high bit set; the shell
may strip it off.  A more reliable way is:

     find . -maxdepth 1 TESTS -okdir rm '{}' \;

where TESTS uniquely identify the file.  The `-maxdepth 1' option
prevents `find' from wasting time searching for the file in any
subdirectories; if there are no subdirectories, you may omit it.  A
good way to uniquely identify the problem file is to figure out its
inode number; use

     ls -i

   Suppose you have a file whose name contains control characters, and
you have found that its inode number is 12345.  This command prompts
you for whether to remove it:

     find . -maxdepth 1 -inum 12345 -okdir rm -f '{}' \;

   If you don't want to be asked, perhaps because the file name may
contain a strange character sequence that will mess up your screen when
printed, then use `-execdir' instead of `-okdir'.

   If you want to rename the file instead, you can use `mv' instead of
`rm':

     find . -maxdepth 1 -inum 12345 -okdir mv '{}' NEW-FILE-NAME \;


File: find.info,  Node: Fixing Permissions,  Next: Classifying Files,  Prev: Strange File Names,  Up: Common Tasks

Fixing Permissions
==================

   Suppose you want to make sure that everyone can write to the
directories in a certain directory tree.  Here is a way to find
directories lacking either user or group write permission (or both),
and fix their permissions:

     find . -type d -not -perm -ug=w | xargs chmod ug+w

You could also reverse the operations, if you want to make sure that
directories do _not_ have world write permission.


File: find.info,  Node: Classifying Files,  Prev: Fixing Permissions,  Up: Common Tasks

Classifying Files
=================

   If you want to classify a set of files into several groups based on
different criteria, you can use the comma operator to perform multiple
independent tests on the files.  Here is an example:

     find / -type d \( -perm -o=w -fprint allwrite , \
       -perm -o=x -fprint allexec \)
     
     echo "Directories that can be written to by everyone:"
     cat allwrite
     echo ""
     echo "Directories with search permissions for everyone:"
     cat allexec

   `find' has only to make one scan through the directory tree (which
is one of the most time consuming parts of its work).


File: find.info,  Node: Worked Examples,  Next: Security Considerations,  Prev: Common Tasks,  Up: Top

Worked Examples
***************

   The tools in the findutils package, and in particular `find', have a
large number of options.  This means that quite often, there is more
than one way to do things.  Some of the options and facilities only
exist for compatibility with other tools, and findutils provides
improved ways of doing things.

   This chapter describes a number of useful tasks that are commonly
performed, and compares the different ways of achieving them.

* Menu:

* Deleting Files::
* Updating A Timestamp File::


File: find.info,  Node: Deleting Files,  Next: Updating A Timestamp File,  Up: Worked Examples

Deleting Files
==============

   One of the most common tasks that `find' is used for is locating
files that can be deleted.  This might include:

   * Files last modified more than 3 years ago which haven't been
     accessed for at least 2 years

   * Files belonging to a certain user

   * Temporary files which are no longer required

   This example concentrates on the actual deletion task rather than on
sophisticated ways of locating the files that need to be deleted.
We'll assume that the files we want to delete are old files underneath
`/var/tmp/stuff'.

The Traditional Way
-------------------

   The traditional way to delete files in `var/tmp/stuff' that have not
been modified in over 90 days would have been:

     find /var/tmp/stuff -mtime +90 -exec /bin/rm {} \;

   The above command uses `-exec' to run the `/bin/rm' command to
remove each file.  This approach works and in fact would have worked in
Version 7 Unix in 1979.  However, there are a number of problems with
this approach.

   The most obvious problem with the approach above is that it causes
`find' to fork every time it finds a file that needs to delete, and the
child process then has to use the `exec' system call to launch
`/bin/rm'.   All this is quite inefficient.  If we are going to use
`/bin/rm' to do this job, it is better to make it delete more than one
file at a time.

   The most obvious way of doing this is to use the shell's command
expansion feature:

     /bin/rm `find /var/tmp/stuff -mtime +90 -print`
   or you could use the more modern form
     /bin/rm $(find /var/tmp/stuff -mtime +90 -print)

   The commands above are much more efficient than the first attempt.
However, there is a problem with them.  The shell has a maximum command
length which is imposed by the operating system (the actual limit
varies between systems).  This means that while the command expansion
technique will usually work, it will suddenly fail when there are lots
of files to delete.  Since the task is to delete unwanted files, this
is precisely the time we don't want things to go wrong.

Making Use of xargs
-------------------

   So, is there a way to be more efficient in the use of `fork()' and
`exec()' without running up against this limit?  Yes, we can be almost
optimally efficient by making use of the `xargs' command.  The `xargs'
command reads arguments from its standard input and builds them into
command lines.  We can use it like this:

     find /var/tmp/stuff -mtime +90 -print | xargs /bin/rm

   For example if the files found by `find' are `/var/tmp/stuff/A',
`/var/tmp/stuff/B' and `/var/tmp/stuff/C' then `xargs' might issue the
commands

     /bin/rm /var/tmp/stuff/A /var/tmp/stuff/B
     /bin/rm /var/tmp/stuff/C

   The above assumes that `xargs' has a very small maximum command line
length.  The real limit is much larger but the idea is that `xargs'
will run `/bin/rm' as many times as necessary to get the job done,
given the limits on command line length.

   This usage of `xargs' is pretty efficient, and the `xargs' command
is widely implemented (all modern versions of Unix offer it).  So far
then, the news is all good.  However, there is bad news too.

Unusual characters in filenames
-------------------------------

   Unix-like systems allow any characters to appear in file names with
the exception of the ASCII NUL character and the backslash.
Backslashes can occur in path names (as the directory separator) but
not in the names of actual directory entries.  This means that the list
of files that `xargs' reads could in fact contain white space
characters -- spaces, tabs and newline characters.  Since by default,
`xargs' assumes that the list of files it is reading uses white space
as an argument separator, it cannot correctly handle the case where a
filename actually includes white space.  This makes the default
behaviour of `xargs' almost useless for handling arbitrary data.

   To solve this problem, GNU findutils introduced the `-print0' action
for `find'.  This uses the ASCII NUL character to separate the entries
in the file list that it produces.  This is the ideal choice of
separator since it is the only character that cannot appear within a
path name.  The `-0' option to `xargs' makes it assume that arguments
are separated with ASCII NUL instead of white space.  It also turns off
another misfeature in the default behaviour of `xargs', which is that
it pays attention to quote characters in its input.  Some versions of
`xargs' also terminate when they see a lone `_' in the input, but GNU
`find' no longer does that (since it has become an optional behaviour
in the Unix standard).

   So, putting `find -print0' together with `xargs -0' we get this
command:

     find /var/tmp/stuff -mtime +90 -print0 | xargs -0 /bin/rm

   The result is an efficient way of proceeding that correctly handles
all the possible characters that could appear in the list of files to
delete.  This is good news.  However, there is, as I'm sure you're
expecting, also more bad news.  The problem is that this is not a
portable construct; although other versions of Unix (notable
BSD-derived ones) support `-print0', it's not universal.  So, is there
a more universal mechanism?

Going back to -exec
-------------------

   There is indeed a more universal mechanism, which is a slight
modification to the `-exec' action.  The normal `-exec' action assumes
that the command to run is terminated with a semicolon (the semicolon
normally has to be quoted in order to protect it from interpretation as
the shell command separator).  The SVR4 edition of Unix introduced a
slight variation, which involves terminating the command with `+'
instead:

     find /var/tmp/stuff -mtime +90 -exec /bin/rm {} \+

   The above use of `-exec' causes `find' to build up a long command
line and then issue it.  This can be less efficient than some uses of
`xargs'; for example `xargs' allows new command lines to be built up
while the previous command is still executing, and allows you to
specify a number of commands to run in parallel.  However, the `find
... -exec ... +' construct has the advantage of wide portability.  GNU
findutils did not support `-exec ... +' until version 4.2.12; one of
the reasons for this is that it already had the `-print0' action in any
case.

A more secure version of -exec
------------------------------

   The command above seems to be efficient and portable.  However,
within it lurks a security problem.  The problem is shared with all the
commands we've tried in this worked example so far, too.  The security
problem is a race condition; that is, if it is possible for somebody to
manipulate the filesystem that you are searching while you are
searching it, it is possible for them to persuade your `find' command
to cause the deletion of a file that you can delete but they normally
cannot.

   The problem occurs because the `-exec' action is defined by the
POSIX standard to invoke its command with the same working directory as
`find' had when it was started.  This means that the arguments which
replace the {} include a relative path from `find''s starting point
down the file that needs to be deleted.  For example,

     find /var/tmp/stuff -mtime +90 -exec /bin/rm {} \+

   might actually issue the command:

     /bin/rm /var/tmp/stuff/A /var/tmp/stuff/B /var/tmp/stuff/passwd

   Notice the file `/var/tmp/stuff/passwd'.  Likewise, the command:

     cd /var/tmp && find stuff -mtime +90 -exec /bin/rm {} \+

   might actually issue the command:

     /bin/rm stuff/A stuff/B stuff/passwd

   If an attacker can rename `stuff' to something else (making use of
their write permissions in `/var/tmp') they can replace it with a
symbolic link to `/etc'.  That means that the `/bin/rm' command will be
invoked on `/etc/passwd'.  If you are running your `find' command as
root, the attacker has just managed to delete a vital file.  All they
needed to do to achieve this was replace a subdirectory with a symbolic
link at the vital moment.

   There is however, a simple solution to the problem.  This is an
action which works a lot like `-exec' but doesn't need to traverse a
chain of directories to reach the file that it needs to work on.  This
is the `-execdir' action, which was introduced by the BSD family of
operating systems.   The command,

     find /var/tmp/stuff -mtime +90 -execdir /bin/rm {} \+

   might delete a set of files by performing these actions:

  1. Change directory to /var/tmp/stuff/foo

  2. Invoke `/bin/rm ./file1 ./file2 ./file3'

  3. Change directory to /var/tmp/stuff/bar

  4. Invoke `/bin/rm ./file99 ./file100 ./file101'

   This is a much more secure method.  We are no longer exposed to a
race condition.  For many typical uses of `find', this is the best
strategy.   It's reasonably efficient, but the length of the command
line is limited not just by the operating system limits, but also by
how many files we actually need to delete from each directory.

   Is it possible to do any better?   In the case of general file
processing, no.  However, in the specific case of deleting files it is
indeed possible to do better.

Using the -delete action
------------------------

   The most efficient and secure method of solving this problem is to
use the `-delete' action:

     find /var/tmp/stuff -mtime +90 -delete

   This alternative is more efficient than any of the `-exec' or
`-execdir' actions, since it entirely avoids the overhead of forking a
new process and using `exec' to run `/bin/rm'.  It is also normally
more efficient than `xargs' for the same reason.   The file deletion is
performed from the directory containing the entry to be deleted, so the
`-delete' action has the same security advantages as the `-execdir'
action has.

   The `-delete' action was introduced by the BSD family of operating
systems.

Improving things still further
------------------------------

   Is it possible to improve things still further?  Not without either
modifying the system library to the operating system or having more
specific knowledge of the layout of the filesystem and disk I/O
subsystem, or both.

   The `find' command traverses the filesystem, reading directories.
It then issues a separate system call for each file to be deleted.  If
we could modify the operating system, there are potential gains that
could be made:

   * We could have a system call to which we pass more than one filename
     for deletion

   * Alternatively, we could pass in a list of inode numbers (on
     GNU/Linux systems, `readdir()' also returns the inode number of
     each directory entry) to be deleted.

   The above possibilities sound interesting, but from the kernel's
point of view it is difficult to enforce standard Unix access controls
for such processing by inode number.  Such a facility would probably
need to be restricted to the superuser.

   Another way of improving performance would be to increase the
parallelism of the process.  For example if the directory hierarchy we
are searching is actually spread across a number of disks, we might
somehow be able to arrange for `find' to process each disk in parallel.
In practice GNU `find' doesn't have such an intimate understanding of
the system's filesystem layout and disk I/O subsystem.

   However, since the system administrator can have such an
understanding they can take advantage of it like so:

     find /var/tmp/stuff1 -mtime +90 -delete &
     find /var/tmp/stuff2 -mtime +90 -delete &
     find /var/tmp/stuff3 -mtime +90 -delete &
     find /var/tmp/stuff4 -mtime +90 -delete &
     wait

   In the example above, four separate instances of `find' are used to
search four subdirectories in parallel.  The `wait' command simply
waits for all of these to complete.  Whether this approach is more or
less efficient than a single instance of `find' depends on a number of
things:

   * Are the directories being searched in parallel actually on separate
     disks?  If not, this parallel search might just result in a lot of
     disk head movement and so the speed might even be slower.

   * Other activity - are other programs also doing things on those
     disks?

Conclusion
----------

   The fastest and most secure way to delete files with the help of
`find' is to use `-delete'.  Using `xargs -0 -P N' can also make
effective use of the disk, but it is not as secure.

   In the case where we're doing things other than deleting files, the
most secure alternative is `-execdir ... +', but this is not as
portable as the insecure action `-exec ... +'.

   The `-delete' action is not completely portable, but the only other
possibillity which is as secure (`-execdir') is no more portable.  The
most efficient portable alternative is `-exec ...+', but this is
insecure and isn't supported by versions of GNU findutils prior to
4.2.12.


File: find.info,  Node: Updating A Timestamp File,  Prev: Deleting Files,  Up: Worked Examples

Updating A Timestamp File
=========================

   Suppose we have a directory full of files which is maintained with a
set of automated tools; perhaps one set of tools updates them and
another set of tools uses the result.  In this situation, it might be
useful for the second set of tools to know if the files have recently
been changed.  It might be useful, for example, to have a 'timestamp'
file which gives the timestamp on the newest file in the collection.

   We can use `find' to achieve this, but there are several different
ways to do it.

Updating the Timestamp The Wrong Way
------------------------------------

   The obvious but wrong answer is just to use `-newer':-

     find subdir -newer timestamp -exec touch -r {} timestamp \;

   This does the right sort of thing but has a bug.  Suppose that two
files in the subdirectory have been updated, and that these are called
`file1' and `file2'.  The command above will update `timestamp' with
the modification time of `file1' or that of `file2', but we don't know
which one.  Since the timestamps on `file1' and `file2' will in general
be different, this could well be the wrong value.

   One solution to this problem is to modify `find' to recheck the
modification time of `timestamp' every time a file is to be compared
against it, but that will reduce the performance of `find'.

Using the test utility to compare timestamps
--------------------------------------------

   The `test' command can be used to compare timestamps:

     find subdir -exec test {} -nt timestamp \; -exec touch -r {} timestamp \;

   This will ensure that any changes made to the modification time of
`timestamp' that take place during the execution of `find' are taken
into account.  This resolves our earlier problem, but unfortunately
this runs much more slowly.

A combined approach
-------------------

   We can of course still use `-newer' to cut down on the number of
calls to `test':

     find subdir -newer timestamp -a \
          -exec test {} -nt timestamp \; -a \
          -exec touch -r {} timestamp \;

   Here, the `-newer' test excludes all the files which are definitely
older than the timestamp, but all the files which are newer than the
old value of the timestamp are compared against the current updated
timestamp.

   This is indeed faster in general, but the speed difference will
depend on how many updated files there are.

Using -printf and sort to compare timestamps
--------------------------------------------

   It is possible to use the `-printf' action to abandon the use of
`test' entirely:

     newest=$(find subdir -newer timestamp -printf "%A%p\n" |
                sort -n |
                tail -1 |
                cut -d: -f2- )
     touch -r "${newest:-timestamp}" timestamp

   The command above works by generating a list of the timestamps and
names of all the files which are newer than the timestamp.  The `sort',
`tail' and `cut' commands simply pull out the name of the file with the
largest timestamp value (that is, the latest file).  The `touch'
command is then used to update the timestamp,

   The `"${newest:-timestamp}"' expression simply expands to the value
of `$newest' if that variable is set, but to `timestamp' otherwise.
This ensures that an argument is always given to the `-r' option of the
`touch' command.

   This approach seems quite efficient, but unfortunately it has a bug.
Many operating systems now keep file modification time information at a
granularity which is finer than one second.  Unfortunately the `%A@'
format for `-printf' only prints a whole-number value currently; that
is, these values are at a one-second granularity.  This means that in
our example above, `$newest' will be the name of a file which is no
more than one second older than the newest file, but may indeed be
older.

   It would be possible to solve this problem with some kind of loop:

     while true; do
             newest=$(find subdir -newer timestamp -printf "%A@:%p\n" |
                sort -n |
                tail -1 |
                cut -d: -f2- )
             if test -z "$newest" ; then
                     break
             else
                     touch -r "$newest" timestamp
             fi
     done

   A better fix for this problem would be to allow the `%A@' format to
produce a result having a fractional part, too.  While this is planned
for GNU `find', it hasn't been done yet.

Coping with sub-second timestamp resolution
-------------------------------------------

   Another tool which often works with timestamps is `make'.  We can
use `find' to generate a `Makefile' file on the fly and then use `make'
to update the timestamps:

     makefile=$(mktemp)
     find subdir \
     	\( \! -xtype l \) \
     	-newer timestamp \
     	-printf "timestamp:: %p\n\ttouch -r %p timestamp\n\n" > "$makefile"
     make -f "$makefile"
     rm   -f "$makefile"

   Unfortunately although the solution above is quite elegant, it fails
to cope with white space within file names, and adjusting it to do so
would require a rather complex shell script.

Coping with odd filenames too
-----------------------------

   We can fix both of these problems (looping and problems with white
space), and do things more efficiently too.  The following command
works with newlines and doesn't need to sort the list of filenames.

     find subdir -newer timestamp -printf "%A@:%p\0" |
        perl -0 newest.pl |
        xargs --no-run-if-empty --null -i \
           find {} -maxdepth 0 -newer timestamp -exec touch -r {} timestamp \;

   The first `find' command generates a list of files which are newer
than the original timestamp file, and prints a list of them with their
timestamps.  The `newest.pl' script simply filters out all the
filenames which have timestamps which are older than whatever the
newest file is:-


     #! /usr/bin/perl -0
     my @newest = ();
     my $latest_stamp = undef;
     while (<>) {
         my ($stamp, $name) = split(/:/);
         if (!defined($latest_stamp) || ($tstamp > $latest_stamp)) {
             $latest_stamp = $stamp;
             @newest = ();
         }
         if ($tstamp >= $latest_stamp) {
             push @newest, $name;
         }
     }
     print join("\0", @newest);

   This prints a list of zero or more files, all of which are newer than
the original timestamp file, and which have the same timestamp as each
other, to the nearest second.  The second `find' command takes each
resulting file one at a time, and if that is newer than the timestamp
file, the timestamp is updated.


File: find.info,  Node: Security Considerations,  Next: Error Messages,  Prev: Worked Examples,  Up: Top

Security Considerations
***********************

   Security considerations are important if you are using `find' or
`xargs' to search for or process files that don't belong to you or
which other people have control.  Security considerations relating to
`locate' may also apply if you have files which you do not want others
to see.

   The most severe forms of security problems affecting `find' and
related programs are when third parties bring about a situation
allowing them to do something they would normally not be able to
accomplish.  This is called _privilege elevation_.  This might include
deleting files they would not normally be able to delete.  It is common
for the operating system to periodically invoke `find' for
self-maintenance purposes.  These invocations of `find' are
particularly problematic from a security point of view as these are
often invoked by the superuser and search the entire filesystem
hierarchy.  Generally, the severity of any associated problem depends
on what the system is going to do with the files found by `find'.

* Menu:

* Levels of Risk::      What is your level of exposure to security problems?
* Security Considerations for find::  Security problems with find
* Security Considerations for xargs:: Security problems with xargs
* Security Considerations for locate:: Security problems with locate
* Security Summary:: That was all very complex, what does it boil down to?


File: find.info,  Node: Levels of Risk,  Next: Security Considerations for find,  Up: Security Considerations

Levels of Risk
==============

   There are some security risks inherent in the use of `find', `xargs'
and (to a lesser extent) `locate'.  The severity of these risks depends
on what sort of system you are using:

*High risk*
     Multi-user systems where you do not control (or trust) the other
     users, and on which you execute `find', including areas where
     those other users can manipulate the filesystem (for example
     beneath `/home' or `/tmp').

*Medium Risk*
     Systems where the actions of other users can create file names
     chosen by them, but to which they don't have access while `find' is
     being run.  This access might include leaving programs running
     (shell background jobs, `at' or `cron' tasks, for example).  On
     these sorts of systems, carefully written commands (avoiding use of
     `-print' for example) should not expose you to a high degree of
     risk.  Most systems fall into this category.

*Low Risk*
     Systems to which untrusted parties do not have access, cannot
     create file names of their own choice (even remotely) and which
     contain no security flaws which might enable an untrusted third
     party to gain access.  Most systems do not fall into this category
     because there are many ways in which external parties can affect
     the names of files that are created on your system.  The system on
     which I am writing this for example automatically downloads
     software updates from the Internet; the names of the files in
     which these updates exist are chosen by third parties(1).

   In the discussion above, "risk" denotes the likelihood that someone
can cause `find', `xargs', `locate' or some other program which is
controlled by them to do something you did not intend.  The levels of
risk suggested do not take any account of the consequences of this sort
of event.  That is, if you operate a "low risk" type system, but the
consequences of a security problem are disastrous, then you should
still give serious thought to all the possible security problems, many
of which of course will not be discussed here - this section of the
manual is intended to be informative but not comprehensive or
exhaustive.

   If you are responsible for the operation of a system where the
consequences of a security problem could be very important, you should
do two things:-

  1. Define a security policy which defines who is allowed to do what
     on your system.

  2. Seek competent advice on how to enforce your policy, detect
     breaches of that policy, and take account of any potential problems
     that might fall outside the scope of your policy.

   ---------- Footnotes ----------

   (1) Of course, I trust these parties to a large extent anyway,
because I install software provided by them; I choose to trust them in
this way, and that's a deliberate choice


File: find.info,  Node: Security Considerations for find,  Next: Security Considerations for xargs,  Prev: Levels of Risk,  Up: Security Considerations

Security Considerations for `find'
==================================

   Some of the actions `find' might take have a direct effect; these
include `-exec' and `-delete'.  However, it is also common to use
`-print' explicitly or implicitly, and so if `find' produces the wrong
list of file names, that can also be a security problem; consider the
case for example where `find' is producing a list of files to be
deleted.

   We normally assume that the `find' command line expresses the file
selection criteria and actions that the user had in mind - that is, the
command line is "trusted" data.

   From a security analysis point of view, the output of `find' should
be correct; that is, the output should contain only the names of those
files which meet the user's criteria specified on the command line.
This applies for the `-exec' and `-delete' actions; one can consider
these to be part of the output.

   On the other hand, the contents of the filesystem can be manipulated
by other people, and hence we regard this as "untrusted" data.  This
implies that the `find' command line is a filter which converts the
untrusted contents of the filesystem into a correct list of output
files.

   The filesystem will in general change while `find' is searching it;
in fact, most of the potential security problems with `find' relate to
this issue in some way.

   "Race conditions" are a general class of security problem where the
relative ordering of actions taken by `find' (for example) and
something else are critically important in getting the correct and
expected result(1) .

   For `find', an attacker might move or rename files or directories in
the hope that an action might be taken against a file which was not
normally intended to be affected.  Alternatively, this sort of attack
might be intended to persuade `find' to search part of the filesystem
which would not normally be included in the search (defeating the
`-prune' action for example).

* Menu:

* Changing the Current Working Directory::
* Race Conditions with -exec::
* Race Conditions with -print and -print0::

   ---------- Footnotes ----------

   (1) This is more or less the definition of the term "race condition"


File: find.info,  Node: Changing the Current Working Directory,  Next: Race Conditions with -exec,  Up: Security Considerations for find

Changing the Current Working Directory
--------------------------------------

   As `find' searches the filesystem, it finds subdirectories and then
searches within them by changing its working directory.  First, `find'
reaches and recognises a subdirectory.  It then decides if that
subdirectory meets the criteria for being searched; that is, any
`-xdev' or `-prune' expressions are taken into account.  The `find'
program will then change working directory and proceed to search the
directory.

   A race condition attack might take the form that once the checks
relevant to `-xdev' and `-prune' have been done, an attacker might
rename the directory that was being considered, and put in its place a
symbolic link that actually points somewhere else.

   The idea behind this attack is to fool `find' into going into the
wrong directory.  This would leave `find' with a working directory
chosen by an attacker, bypassing any protection apparently provided by
`-xdev' and `-prune', and any protection provided by being able to
_not_ list particular directories on the `find' command line.  This
form of attack is particularly problematic if the attacker can predict
when the `find' command will be run, as is the case with `cron' tasks
for example.

   GNU `find' has specific safeguards to prevent this general class of
problem.  The exact form of these safeguards depends on the properties
of your system.

* Menu:

* O_NOFOLLOW::                     Safely changing directory using fchdir().
* Systems without O_NOFOLLOW::     Checking for symbolic links after chdir().


File: find.info,  Node: O_NOFOLLOW,  Next: Systems without O_NOFOLLOW,  Up: Changing the Current Working Directory

O_NOFOLLOW
..........

   If your system supports the O_NOFOLLOW flag (1) to the `open(2)'
system call, `find' uses it when safely changing directory.  The target
directory is first opened and then `find' changes working directory
with the `fchdir()' system call.  This ensures that symbolic links are
not followed, preventing the sort of race condition attack in which use
is made of symbolic links.

   If for any reason this approach does not work, `find' will fall back
on the method which is normally used if O_NOFOLLOW is not supported.

   You can tell if your system supports O_NOFOLLOW by running

     find --version

   This will tell you the version number and which features are enabled.
For example, if I run this on my system now, this gives:
     GNU find version 4.2.18-CVS
     Features enabled: D_TYPE O_NOFOLLOW(enabled)

   Here, you can see that I am running a version of `find' which was
built from the development (CVS) code prior to the release of
findutils-4.2.18, and that the D_TYPE and O_NOFOLLOW features are
present.  O_NOFOLLOW is qualified with "enabled".  This simply means
that the current system seems to support O_NOFOLLOW.  This check is
needed because it is possible to build `find' on a system that defines
O_NOFOLLOW and then run it on a system that ignores the O_NOFOLLOW
flag.  We try to detect such cases at startup by checking the operating
system and version number; when this happens you will see
"O_NOFOLLOW(disabled)" instead.

   ---------- Footnotes ----------

   (1) GNU/Linux (kernel version 2.1.126 and later) and FreeBSD
(3.0-CURRENT and later) support this


File: find.info,  Node: Systems without O_NOFOLLOW,  Prev: O_NOFOLLOW,  Up: Changing the Current Working Directory

Systems without O_NOFOLLOW
..........................

   The strategy for preventing this type of problem on systems that lack
support for the O_NOFOLLOW flag is more complex.  Each time `find'
changes directory, it examines the directory it is about to move to,
issues the `chdir()' system call, and then checks that it has ended up
in the subdirectory it expected.  If all is as expected, processing
continues as normal.  However, there are two main reasons why the
directory might change: the use of an automounter and the someone
removing the old directory and replacing it with something else while
`find' is trying to descend into it.

   Where a filesystem "automounter" is in use it can be the case that
the use of the `chdir()' system call can itself cause a new filesystem
to be mounted at that point.  On systems that do not support
O_NOFOLLOW, this will cause `find''s security check to fail.

   However, this does not normally represent a security problem, since
the automounter configuration is normally set up by the system
administrator.  Therefore, if the `chdir()' sanity check fails, `find'
will make one more attempt.  If that succeeds, execution carries on as
normal.  This is the usual case for automounters.

   Where an attacker is trying to exploit a race condition, the problem
may not have gone away on the second attempt.  If this is the case,
`find' will issue a warning message and then ignore that subdirectory.
When this happens, actions such as `-exec' or `-print' may already have
taken place for the problematic subdirectory.  This is because `find'
applies tests and actions to directories before searching within them
(unless `-depth' was specified).

   Because of the nature of the directory-change operation and security
check, in the worst case the only things that `find' would have done
with the directory are to move into it and back out to the original
parent.  No operations would have been performed within that directory.


File: find.info,  Node: Race Conditions with -exec,  Next: Race Conditions with -print and -print0,  Prev: Changing the Current Working Directory,  Up: Security Considerations for find

Race Conditions with -exec
--------------------------

   The `-exec' action causes another program to be run.  It passes to
the program the name of the file which is being considered at the time.
The invoked program will typically then perform some action on that
file.  Once again, there is a race condition which can be exploited
here.  We shall take as a specific example the command

     find /tmp -path /tmp/umsp/passwd -exec /bin/rm

   In this simple example, we are identifying just one file to be
deleted and invoking `/bin/rm' to delete it.  A problem exists because
there is a time gap between the point where `find' decides that it
needs to process the `-exec' action and the point where the `/bin/rm'
command actually issues the `unlink()' system call to delete the file
from the filesystem.  Within this time period, an attacker can rename
the `/tmp/umsp' directory, replacing it with a symbolic link to `/etc'.
There is no way for `/bin/rm' to determine that it is working on the
same file that `find' had in mind.  Once the symbolic link is in place,
the attacker has persuaded `find' to cause the deletion of the
`/etc/passwd' file, which is not the effect intended by the command
which was actually invoked.

   One possible defence against this type of attack is to modify the
behaviour of `-exec' so that the `/bin/rm' command is run with the
argument `./passwd' and a suitable choice of working directory.  This
would allow the normal sanity check that `find' performs to protect
against this form of attack too.  Unfortunately, this strategy cannot
be used as the POSIX standard specifies that the current working
directory for commands invoked with `-exec' must be the same as the
current working directory from which `find' was invoked.  This means
that the `-exec' action is inherently insecure and can't be fixed.

   GNU `find' implements a more secure variant of the `-exec' action,
`-execdir'.  The `-execdir' action ensures that it is not necessary to
dereference subdirectories to process target files.  The current
directory used to invoke programs is the same as the directory in which
the file to be processed exists (`/tmp/umsp' in our example, and only
the basename of the file to be processed is passed to the invoked
command, with a `./' prepended (giving `./passwd' in our example).

   The `-execdir' action refuses to do anything if the current
directory is included in the $PATH environment variable.  This is
necessary because `-execdir' runs programs in the same directory in
which it finds files - in general, such a directory might be writable
by untrusted users.  For similar reasons, `-execdir' does not allow
`{}' to appear in the name of the command to be run.


File: find.info,  Node: Race Conditions with -print and -print0,  Prev: Race Conditions with -exec,  Up: Security Considerations for find

Race Conditions with -print and -print0
---------------------------------------

   The `-print' and `-print0' actions can be used to produce a list of
files matching some criteria, which can then be used with some other
command, perhaps with `xargs'.  Unfortunately, this means that there is
an unavoidable time gap between `find' deciding that one or more files
meet its criteria and the relevant command being executed.  For this
reason, the `-print' and `-print0' actions are just as insecure as
`-exec'.

   In fact, since the construction

     find ...  -print | xargs ....

   does not cope correctly with newlines or other "white space" in file
names, and copes poorly with file names containing quotes, the `-print'
action is less secure even than `-print0'.

